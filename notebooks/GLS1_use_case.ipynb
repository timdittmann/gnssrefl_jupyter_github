{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gnssrefl.gps as g\n",
    "import gnssrefl.rinex2snr as rnx\n",
    "import gnssrefl.quickLook_function as quick\n",
    "import gnssrefl.gnssir as guts\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import check_parameters\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from csv import reader\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns; sns.set_theme(style=\"whitegrid\");\n",
    "\n",
    "# making sure that env variables are set - if they are then nothing will print to screen\n",
    "g.check_environ_variables()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dye2, Greenland \n",
    "\n",
    "**Station Name:** gls1\n",
    "\n",
    "**Location:**  Dye2, Qeqqata Province, Greenland \n",
    "\n",
    "**Archive:**  [UNAVCO](http://www.unavco.org), [SOPAC](http://sopac-csrc.ucsd.edu/index.php/sopac/)\n",
    "\n",
    "**DOI:**  [https://doi.org/10.7283/T5WS8RDB](https://doi.org/10.7283/T5WS8RDB)\n",
    "\n",
    "**Ellipsoidal Coordinates:**\n",
    "\n",
    "- Latitude: 66.47940\n",
    "\n",
    "- Longitude:  -46.31015\n",
    "\n",
    "- Height: 2150 m\n",
    "\n",
    "[Station Page at UNAVCO](https://www.unavco.org/instrumentation/networks/status/nota/overview/gls1)\n",
    "\n",
    "[Station Page at Nevada Geodetic Laboratory](http://geodesy.unr.edu/NGLStationPages/stations/GLS1.sta)\n",
    "\n",
    "[Google Maps Link](https://goo.gl/maps/391a7h2HpacAa59u8) \n",
    "\n",
    "<img src=\"gls1-photo.png\" width=\"400\">\n",
    "<img src=\"gls1.jpg\" width=\"400\">\n",
    "<BR>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary\n",
    "\n",
    "Station gls1 was installed at [Dye2](http://greenlandtoday.com/dye-2-a-relic-from-a-not-so-distant-past/?lang=en) on the Greenland Ice Sheet in 2011. \n",
    "The antenna is mounted on a long pole; approximately 3.5-meter of the pole was above the ice at the time of installation. \n",
    "The receiver at the site only consistently tracks legacy GPS signals. A detailed discussion of the monument and \n",
    "data from the station can be found in [Larson, MacFerrin, and Nylen (2020)](https://tc.copernicus.org/articles/14/1985/2020/tc-14-1985-2020.pdf). \n",
    "The latest position time series for gls1 can be retrieved \n",
    "from the [Nevada Geodetic Laboratory](http://geodesy.unr.edu/gps_timeseries/tenv3/IGS14/GLS1.tenv3). \n",
    "We also have a utility you can use: **download_unr**\n",
    "\n",
    "As gls1 is on an ice sheet and the ice surface is relatively smooth in all directions, it \n",
    "is unlikely that a complicated azimuth mask will be required.\n",
    "gls1 was originally installed with an elevation mask of 7 degrees, which is suboptimal for reflections research.\n",
    "Even though the mask was later removed, we will use 7 degrees as the minimum elevation angle for all our analysis.\n",
    "Similarly, even though the site managers later changed to enable L2C tracking, to ensure that \n",
    "a consistent dataset is being used, we will only use L1 data. gls1 is an example case \n",
    "for the GNSS-IR Web App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://gnss-reflections.org/api?example=gls1\" width=\"800\" height=\"950\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quickLook \n",
    "\n",
    "Our ultimate goal in this use case is to analyze one year of data. We have chosen the year \n",
    "2012 because there was a large melt event on the ice sheet. In order to set the proper\n",
    "quality control parameters, we will use **quickLook** for one day. First we need to translate \n",
    "one day of RINEX data using **rinex2snr**. We will use day of year 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = 'gls1'\n",
    "year = 2012 \n",
    "doy = 100\n",
    "\n",
    "lat = 66.4794\n",
    "long = -46.3102\n",
    "height = 2148.578"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand what rinex2snr returns, you can uncomment the next line of code to learn more about this function \n",
    "# and it's default parameters\n",
    "# check_parameters.rinex2snr?\n",
    "args = check_parameters.rinex2snr(station, year, doy, translator='fortran')\n",
    "rnx.run_rinex2snr(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a plotting function for the quicklook function\n",
    "def quicklook_results(args, values):\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(10,10))\n",
    "    quadrants = ['NW', 'NE', 'SW', 'SE']\n",
    "    axes = [ax[0,0], ax[0,1], ax[1,0], ax[1,1]]\n",
    "\n",
    "    for i, quadrant in enumerate(quadrants):\n",
    "        satellites = values[quadrant].keys()\n",
    "        fail_satellites = values[f'f{quadrant}'].keys()\n",
    "\n",
    "        for failsat in fail_satellites:\n",
    "            axes[i].plot(values[f'f{quadrant}'][failsat][0], values[f'f{quadrant}'][failsat][1], color='lightgrey') \n",
    "        for sat in satellites:\n",
    "            axes[i].plot(values[quadrant][sat][0], values[quadrant][sat][1])\n",
    "\n",
    "    ax[0,0].set_title('Northwest', size=14)\n",
    "    ax[0,1].set_title('Northeast',size=14)\n",
    "    ax[1,0].set_title('Southwest', size=14)\n",
    "    ax[1,1].set_title('Southeast', size=14)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('reflector height (m)', size=14)\n",
    "        ax.set_ylabel('volts/volts', size=14)\n",
    "        ax.grid()\n",
    "    \n",
    "    fig.suptitle(f'GNSS Station {args[\"station\"].upper()}, {args[\"year\"]} doy {args[\"doy\"]}, freq L1, elevation angles {args[\"e1\"]}-{args[\"e2\"]} \\n', size=16)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def quicklook_metrics(args, values):\n",
    "#     fig, ax = plt.subplots(ncols=1, nrows=3, figsize=(10,10), sharex=True)\n",
    "    quadrants = ['NW', 'NE', 'SW', 'SE']\n",
    "    \n",
    "    # re-organizing the data in a plotting friendly format\n",
    "    success_data = {'Azimuth': [], 'Reflector Height': [], 'Peak to Noise':[], 'Amplitude': []}\n",
    "    fail_data =  {'Azimuth': [], 'Reflector Height': [], 'Peak to Noise': [], 'Amplitude': []}\n",
    "    \n",
    "    for i, quadrant in enumerate(quadrants):\n",
    "        for j in values[quadrant].keys():\n",
    "            success_data['Azimuth'].append(datakeys[quadrant][j][0])\n",
    "            success_data['Reflector Height'].append(datakeys[quadrant][j][1])\n",
    "            success_data['Peak to Noise'].append(datakeys[quadrant][j][5])\n",
    "            success_data['Amplitude'].append(datakeys[quadrant][j][4])\n",
    "        for k in values[f'f{quadrant}'].keys():\n",
    "            fail_data['Azimuth'].append(datakeys[f'f{quadrant}'][k][0])\n",
    "            fail_data['Reflector Height'].append(datakeys[f'f{quadrant}'][k][1])\n",
    "            fail_data['Peak to Noise'].append(datakeys[f'f{quadrant}'][k][5])\n",
    "            fail_data['Amplitude'].append(datakeys[f'f{quadrant}'][k][4])\n",
    "\n",
    "    return pd.DataFrame(success_data), pd.DataFrame(fail_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = check_parameters.quicklook(station, year, doy=doy)\n",
    "values, datakeys = quick.quickLook_function(**args)\n",
    "quicklook_results(args, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a geographically oriented-summary of the frequency content of the GPS data.\n",
    "The peaks in these periodograms tell us how high the GPS antenna is above the ice surface.\n",
    "The peaks are associated with a reflector height (RH) of ~2.5 meters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next plot shows results with respect to azimuth angle.  The top plot is RH and the other \n",
    "two are quality control measures: peak amplitude and peak to noise ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, fail = quicklook_metrics(args, datakeys)\n",
    "fig, axes = plt.subplots(ncols=1, nrows=3, figsize=(10,10), sharex=True)\n",
    "fig.suptitle(f'QuickLook Retrieval Metrics: {args[\"station\"]} GPS L1', size=16)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    g = sns.scatterplot(x='Azimuth',y=success.columns[i+1], data=success, ax=ax, label='good')\n",
    "    g = sns.scatterplot(x='Azimuth',y=fail.columns[i+1], data=fail, ax=ax, color='lightgrey', label='bad')\n",
    "    \n",
    "axes[0].legend(loc='upper right')\n",
    "avg_rh = np.mean(success['Reflector Height'])\n",
    "qc_val_peak2noise = round(min(success['Peak to Noise']))\n",
    "axes[1].axhline(qc_val_peak2noise, linestyle='--', color='black', label='QC value used')\n",
    "qc_val_amp = round(min(success['Amplitude']))\n",
    "axes[2].axhline(qc_val_amp, linestyle='--', color='black', label='QC value used')\n",
    "print(f'Average reflector height value: {avg_rh:.1f}')\n",
    "print('QC value for peak to noise:', qc_val_peak2noise)\n",
    "print('QC value for amplitude:', qc_val_amp)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the top plot we see that the retrieved reflector heights are consistent at all azimuths.\n",
    "Retrievals for azimuths between 340 degrees and 40 degrees are consistently marked as not having\n",
    "met quality control settings.From the center plot we can see that a peak2noise QC metric of 3 is reasonable. \n",
    "Similarly, the amplitudes (bottom plot) are generally larger than 10, so 8 is an acceptable minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to make SNR files for the year 2012:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = check_parameters.rinex2snr(station,year,1, doy_end=366, translator='fortran')\n",
    "rnx.run_rinex2snr(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next analyze a year of L1 GPS reflection data from this site. We will use the default minimum and maximum \n",
    "reflector height values (0.4 and 6 meters). But for the reasons previously stated, we will set a minimum elevation angle \n",
    "of 7 degrees. We also specify that we only want to use the L1 data and set peak2noise and a mimimum\n",
    "amplitude for the periodograms. We use the utility **make_json_input** to set and store these analysis settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_parameters.make_json(station, lat, long, height, e1=7, peak2noise=3, ampl=8, l1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the json file that was created\n",
    "json_file = 'input/gls1.json'\n",
    "with open(json_file, \"r\") as myfile:\n",
    "    file = json.load(myfile)\n",
    "    file['azval'] = [40,90,90,180,180,270,270,330]\n",
    "os.remove(json_file)\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(file, f, indent=4)\n",
    "    \n",
    "with open(json_file, \"r\") as myfile:\n",
    "    file = json.load(myfile)\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also excluded a bit of the northern tracks by hand-editing the json. This is not required as \n",
    "the software appears to be appropriately removing these unreliable azimuths. Note: the removal of these\n",
    "azimuths is more related to the GPS satellite inclination than local conditions at gls1.\n",
    "\n",
    "Now that you have SNR files and json inputs, you can go ahead and estimate reflector heights for the year 2012:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2012\n",
    "doy = 1\n",
    "doy_end = 366\n",
    "plt=False\n",
    "args = check_parameters.gnssir(station, year, doy, doy_end=doy_end, plt=plt, screenstats=False)\n",
    "year_list = list(range(year, args['year_end'] + 1))\n",
    "doy_list = list(range(doy, args['doy_end'] + 1))\n",
    "for year in year_list:\n",
    "    args['args']['year'] = year\n",
    "    for doy in doy_list:\n",
    "        args['args']['doy'] = doy\n",
    "        guts.gnssir_guts(**args['args'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **daily_avg** tool to compute a daily average RH. A median filter is set to 0.25 meters \n",
    "and 30 individual tracks are required in order to recover a daily average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_parameters.daily_avg(station, medfilter=.25, ReqTracks=30, plt2screen=False, txtfile='gls1-dailyavg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def read_allRH_file(filepath, regex):\n",
    "    data = {'dates': [], 'rh': []}\n",
    "    #read daily average reflector heights\n",
    "    with open(f'{refl_code_dir}{filepath}', 'r') as myfile:\n",
    "        file = myfile.read()\n",
    "        matches = re.finditer(regex, file, flags=re.MULTILINE)\n",
    "\n",
    "        for match in matches:\n",
    "            ydoy = f'{int(match.group(\"year\"))}-{int(match.group(\"doy\"))}'\n",
    "            date = datetime.strptime(ydoy, '%Y-%j').date()\n",
    "            data['dates'].append(date)\n",
    "            data['rh'].append(float(match.group('rh')))\n",
    "            \n",
    "    return data\n",
    "\n",
    "regex = '^ (?P<year>[ \\d]+) +(?P<doy>[\\d]+) +(?P<rh>[\\d|-|.]+)'\n",
    "filepath = f'/Files/{station}_allRH.txt'\n",
    "data = read_allRH_file(filepath, regex)\n",
    "\n",
    "df = pd.DataFrame(data, index=None, columns=['dates', 'rh'])\n",
    "plt.figure(figsize=(8,8))\n",
    "g = sns.scatterplot(x='dates', y='rh', data=df, hue='dates', palette='colorblind', legend=False)\n",
    "g.set_ylabel('Reflector Height (m)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "df_group = df.groupby(['dates']).agg(['count'])\n",
    "g = sns.scatterplot(data=df_group)\n",
    "g.set_title('Number of values used in the daily average', size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '^ (?P<year>[ \\d]+) +(?P<doy>[\\d]+) +(?P<rh>[\\d|-|.]+)'\n",
    "filepath = f'/Files/{station}-dailyavg.txt'\n",
    "data = read_allRH_file(filepath, regex)\n",
    "df = pd.DataFrame(data, index=None, columns=['dates', 'rh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "g = sns.scatterplot(x='dates', y='rh', data=df, legend=False)\n",
    "g.set_ylim(3.6,2.3)\n",
    "g.set_ylabel('Reflector Height (m)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three plots are returned. The first is all tracks. The second shows the number of tracks used in the daily average.Finally, the average RH each day for the year 2012.\n",
    "\n",
    "This data shown in the last plot show you long-term accumulation as well as relatively small snow accumulation events. The overall \n",
    "plot is dominated by the large melt event in the summer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to think about:**\n",
    "\n",
    "* Why do the number of useable tracks drop drastically at various times in the year?\n",
    "\n",
    "* Why are the number of tracks retrieved in the summer days consistently higher in number than \n",
    "in other times of the year? What is different about the surface in the summer of 2012?\n",
    "\n",
    "* How would you find out whether this year was anomalously large melt?  \n",
    "\n",
    "* Try comparing the GNSS-IR results with the [validation data](https://tc.copernicus.org/articles/14/1985/2020/tc-14-1985-2020.pdf)\n",
    "\n",
    "The original [J. Glaciology paper](https://www.kristinelarson.net/wp-content/uploads/2015/10/LarsonWahrKuipers_2015.pdf ) discussing this site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnssrefl_jupyter",
   "language": "python",
   "name": "gnssrefl_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
