{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gnssrefl.gps as g\n",
    "import gnssrefl.rinex2snr as rnx\n",
    "import gnssrefl.quickLook_function as quick\n",
    "import gnssrefl.gnssir as guts\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import check_parameters\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from csv import reader\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns; sns.set_theme(style=\"whitegrid\");\n",
    "\n",
    "# making sure that env variables are set - if they are then nothing will print to screen\n",
    "g.check_environ_variables()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Michipicoten, Lake Superior\n",
    "\n",
    "**Station Name:** \tmchn\n",
    "\n",
    "**Location:** Michipicoten Harbour, Ontario, Canada\n",
    "\n",
    "**Archive:**  [SOPAC](http://sopac-csrc.ucsd.edu/index.php/sopac/), [NRCAN](https://www.nrcan.gc.ca/home)\n",
    "\n",
    "**Ellipsoidal Coordinates:**\n",
    "\n",
    "- Latitude: 47.961\n",
    "\n",
    "- Longitude: -84.901\n",
    "\n",
    "- Height: 152.019 m\n",
    "\n",
    "[Station Page at Natural Resources Canada](https://webapp.geod.nrcan.gc.ca/geod/data-donnees/station/report-rapport.php?id=M093001)\n",
    "\n",
    "[Station Page at Nevada Geodetic Laboratory](http://geodesy.unr.edu/NGLStationPages/stations/MCHN.sta)\n",
    "\n",
    "[Google Maps Link](https://goo.gl/maps/mU5GbsvMsLfe5buQ7)\n",
    "\n",
    "<img src=\"mchn_monu-cors.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary\n",
    "\n",
    "Station mchn is operated by NRCAN. The station overlooks Lake Superior from the Canadian shore, in a favorable location for measuring seasonal water levels.\n",
    "\n",
    "This site only tracks GPS. Unfortunately only L1 data should be used at this site. The L2 data at mchn do not meet basic quality standards, and the L5 signal is not tracked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on the station can be obtained from the GNSS-IR Web App, where mchn is one of the [test cases](https://gnss-reflections.org/api?example=mchn) for water level.\n",
    "The web app returns a photograph, coordinates (make a note of them),\n",
    "a Google Earth map, and a periodogram which we will reproduce in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://gnss-reflections.org/api?example=mchn\" width=\"800\" height=\"950\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Azimuth and Elevation Mask\n",
    "\n",
    "From the periodogram and Google Earth a good \n",
    "azimuth mask can be selected.  Elevation angle might be a bit trickier, but in this case, go ahead and \n",
    "use the default values included in the title of the periodogram plot from the web app. The web app has an option to calculate the reflection zones (\"ReflZones\" link located in the bar at the top of the page). [Below is a good start on an elevation and azimuth angle mask](https://gnss-reflections.org/rzones?station=mchn&msl=on&RH=7&eang=2&azim1=80&azim2=180). The reflection zones at 5, 10, and 15-degree elevation angles are plotted as colored ellipses surrounding the station, all overlaid on a Google Earth map.  Higher elevation angles are closer to the station.  From the map, it is possible to pick out the azimuth range and reflection zones that are over water instead of than land."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://gnss-reflections.org/rzones?station=mchn&msl=on&RH=7&eang=2&azim1=80&azim2=180\" width=\"950\" height=\"700\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce the Web App \n",
    "\n",
    "### Make SNR File\n",
    "\n",
    "If you know where the data are stored (i.e. sopac), it is better (faster) to set that flag.\n",
    "Since the receiver only tracks GPS signals, there is no need to specify gnss orbits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = 'sbas'\n",
    "year = 2018 \n",
    "doy = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand what rinex2snr returns, you can uncomment the next line of code to learn more about this function \n",
    "# and it's default parameters\n",
    "check_parameters.rinex2snr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = check_parameters.rinex2snr(station, year, doy, archive='special', translator='hybrid', orb='igs')\n",
    "rnx.run_rinex2snr(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Look at Data\n",
    "\n",
    "Use **quickLook** to examine the spectral characteristics of the SNR data for the default L1 settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a plotting function for the quicklook function\n",
    "def quicklook_results(args, values):\n",
    "    freq = {1:'L1', 20: 'L2C'}\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(10,10))\n",
    "    quadrants = ['NW', 'NE', 'SW', 'SE']\n",
    "    axes = [ax[0,0], ax[0,1], ax[1,0], ax[1,1]]\n",
    "\n",
    "    for i, quadrant in enumerate(quadrants):\n",
    "        satellites = values[quadrant].keys()\n",
    "        fail_satellites = values[f'f{quadrant}'].keys()\n",
    "\n",
    "        for failsat in fail_satellites:\n",
    "            axes[i].plot(values[f'f{quadrant}'][failsat][0], values[f'f{quadrant}'][failsat][1], color='lightgrey') \n",
    "        for sat in satellites:\n",
    "            axes[i].plot(values[quadrant][sat][0], values[quadrant][sat][1])\n",
    "\n",
    "    ax[0,0].set_title('Northwest', size=14)\n",
    "    ax[0,1].set_title('Northeast',size=14)\n",
    "    ax[1,0].set_title('Southwest', size=14)\n",
    "    ax[1,1].set_title('Southeast', size=14)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('reflector height (m)', size=14)\n",
    "        ax.set_ylabel('volts/volts', size=14)\n",
    "        ax.grid()\n",
    "    \n",
    "    fig.suptitle(f'GNSS Station {args[\"station\"].upper()}, {args[\"year\"]} doy {args[\"doy\"]}, freq {freq[args[\"f\"]]}, elevation angles {args[\"e1\"]}-{args[\"e2\"]} \\n', size=16)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def quicklook_metrics(args, values):\n",
    "#     fig, ax = plt.subplots(ncols=1, nrows=3, figsize=(10,10), sharex=True)\n",
    "    quadrants = ['NW', 'NE', 'SW', 'SE']\n",
    "    \n",
    "    # re-organizing the data in a plotting friendly format\n",
    "    success_data = {'Azimuth': [], 'Reflector Height': [], 'Peak to Noise':[], 'Amplitude': []}\n",
    "    fail_data =  {'Azimuth': [], 'Reflector Height': [], 'Peak to Noise': [], 'Amplitude': []}\n",
    "    \n",
    "    for i, quadrant in enumerate(quadrants):\n",
    "        for j in values[quadrant].keys():\n",
    "            success_data['Azimuth'].append(datakeys[quadrant][j][0])\n",
    "            success_data['Reflector Height'].append(datakeys[quadrant][j][1])\n",
    "            success_data['Peak to Noise'].append(datakeys[quadrant][j][5])\n",
    "            success_data['Amplitude'].append(datakeys[quadrant][j][4])\n",
    "        for k in values[f'f{quadrant}'].keys():\n",
    "            fail_data['Azimuth'].append(datakeys[f'f{quadrant}'][k][0])\n",
    "            fail_data['Reflector Height'].append(datakeys[f'f{quadrant}'][k][1])\n",
    "            fail_data['Peak to Noise'].append(datakeys[f'f{quadrant}'][k][5])\n",
    "            fail_data['Amplitude'].append(datakeys[f'f{quadrant}'][k][4])\n",
    "\n",
    "    return pd.DataFrame(success_data), pd.DataFrame(fail_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand what quicklook returns, you can uncomment the next line of code to learn more about this function \n",
    "# and it's default parameters\n",
    "# check_parameters.quicklook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = check_parameters.quicklook(station, year, doy=doy)\n",
    "values, datakeys = quick.quickLook_function(**args)\n",
    "quicklook_results(args, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this not look like the results from the web app? Look closely at the station photo and the x-axis of the periodograme, then change the range of reflector heights at the command line for **quickLook**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the range of reflector heights frome the default to 2-8\n",
    "args = check_parameters.quicklook(station, year, doy=doy, h1=2, h2=8)\n",
    "values, datakeys = quick.quickLook_function(**args)\n",
    "\n",
    "quicklook_results(args, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also look at the QC metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, fail = quicklook_metrics(args, datakeys)\n",
    "fig, axes = plt.subplots(ncols=1, nrows=3, figsize=(10,10), sharex=True)\n",
    "fig.suptitle(f'QuickLook Retrieval Metrics: {args[\"station\"]} GPS L1', size=16)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    g = sns.scatterplot(x='Azimuth',y=success.columns[i+1], data=success, ax=ax, label='good')\n",
    "    g = sns.scatterplot(x='Azimuth',y=fail.columns[i+1], data=fail, ax=ax, color='lightgrey', label='bad')\n",
    "    \n",
    "axes[0].legend(loc='upper right')\n",
    "avg_rh = np.mean(success['Reflector Height'])\n",
    "qc_val_peak2noise = round(min(success['Peak to Noise']))\n",
    "axes[1].axhline(qc_val_peak2noise, linestyle='--', color='black', label='QC value used')\n",
    "qc_val_amp = round(min(success['Amplitude']))\n",
    "axes[2].axhline(qc_val_amp, linestyle='--', color='black', label='QC value used')\n",
    "print(f'Average reflector height value: {avg_rh:.1f}')\n",
    "print('QC value for peak to noise:', qc_val_peak2noise)\n",
    "print('QC value for amplitude:', qc_val_amp)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The water is ~6.5 meters below the antenna. You can see from the top plot that the good retrievals (blue dots) very clearly show you which azimuths are acceptable and which are not. The middle plot shows the peak to noise ratio, which we would like to at least exceed 3. Here, the bad retrievals are always below this level. The amplitudes in the bottom plot indicate that 8 is an acceptable minimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Data\n",
    "\n",
    "The data from 2013 will be analyzed for this use case.  Begin by generating the SNR files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = 'mchn'\n",
    "year = 2013 \n",
    "doy = 1\n",
    "\n",
    "lat = 47.961\n",
    "long = -84.901\n",
    "height = 152.019\n",
    "\n",
    "args = check_parameters.rinex2snr(station,year,doy, doy_end=365, archive='sopac', translator='hybrid')\n",
    "rnx.run_rinex2snr(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting files are stored in $REFL_CODE/2013/snr/mchn. REFL_CODE is set to this current directory.\n",
    "\n",
    "Analysis parameters are set up with **make_json_input**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand what make_json returns, you can uncomment the next line of code to learn more about this function \n",
    "# and it's default parameters\n",
    "#check_parameters.make_json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_parameters.make_json(station, lat, long, height, h1=3, h2=10, peak2noise=3, ampl=8, l1=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most of the analysis settings can be set in the make_json function, we must set the azimuths by 'hand' to be limited to 80-180 degrees. Although it is possible to get good reflections beyond 180 degrees, the photographs suggest barriers are present in that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the json file that was created\n",
    "json_file = 'input/mchn.json'\n",
    "with open(json_file, \"r\") as myfile:\n",
    "    file = json.load(myfile)\n",
    "    file['azval'] = [80, 180]\n",
    "\n",
    "os.remove(json_file)\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(file, f, indent=4)\n",
    "    \n",
    "with open(json_file, \"r\") as myfile:\n",
    "    file = json.load(myfile)\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the analysis parameters are set, run **gnssir** to save the reflector height (RH) output for each day in 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand what gnssir returns, you can uncomment the next line of code to learn more about this function \n",
    "# and it's default parameters\n",
    "# check_parameters.gnssir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doy = 1\n",
    "doy_end=365\n",
    "args = check_parameters.gnssir(station, year, doy=1, doy_end=365, plt=False, screenstats=False)\n",
    "\n",
    "year_list = list(range(year, args['year_end'] + 1))\n",
    "doy_list = list(range(doy, args['doy_end'] + 1))\n",
    "for year in year_list:\n",
    "    args['args']['year'] = year\n",
    "    for doy in doy_list:\n",
    "        args['args']['doy'] = doy\n",
    "        guts.gnssir_guts(**args['args'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The daily output files are stored in $REFL_CODE/2013/results/mchn. There is an option to plot periodograms, but for a year's worth of data this is not recommended.  Instead it is better to look at the periodograms for a single day.  Separate periodograms will be generated for all frequency codes listed in the json file, but only those frequency for which SNR data are available will have spectra plotted, i.e., no data will produce a blank plot.  The messages printed to the screen can also be reduced with the screenstats flag (this option can be manually set in the json file too):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doy = 195\n",
    "args = check_parameters.gnssir(station, year, doy=doy, plt=True, screenstats=False)\n",
    "\n",
    "year_list = list(range(year, args['year_end'] + 1))\n",
    "doy_list = list(range(doy, args['doy_end'] + 1))\n",
    "print(doy_list)\n",
    "for year in year_list:\n",
    "    args['args']['year'] = year\n",
    "    for doy in doy_list:\n",
    "        args['args']['doy'] = doy\n",
    "        guts.gnssir_guts(**args['args'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still outliers in the solutions - and in principle these can be reduced by experimenting with better restrictions, i.e. increase the amplitude requirement or peak to noise restriction. If the outliers are not taken into account, they will show up in the daily average:\n",
    "\n",
    "Let's set the median filter to allow any value within 2 meters of the median. The number of tracks needed to compute an average will be 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand what daily_avg returns, you can uncomment the next line of code to learn more about this function \n",
    "# and it's default parameters\n",
    "# check_parameters.daily_avg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_parameters.daily_avg(station, medfilter=2, ReqTracks=10, plt2screen=False, txtfile='mchn-dailyavg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_allRH_file(filepath, regex):\n",
    "    data = {'dates': [], 'rh': []}\n",
    "    #read daily average reflector heights\n",
    "    with open(f'{refl_code_dir}{filepath}', 'r') as myfile:\n",
    "        file = myfile.read()\n",
    "        matches = re.finditer(regex, file, flags=re.MULTILINE)\n",
    "\n",
    "        for match in matches:\n",
    "            ydoy = f'{int(match.group(\"year\"))}-{int(match.group(\"doy\"))}'\n",
    "            date = datetime.strptime(ydoy, '%Y-%j').date()\n",
    "            data['dates'].append(date)\n",
    "            data['rh'].append(float(match.group('rh')))\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '^ (?P<year>[ \\d]+) +(?P<doy>[\\d]+) +(?P<rh>[\\d|-|.]+)'\n",
    "filepath = f'/Files/{station}_allRH.txt'\n",
    "data = read_allRH_file(filepath, regex)\n",
    "\n",
    "df = pd.DataFrame(data, index=None, columns=['dates', 'rh'])\n",
    "plt.figure(figsize=(8,8))\n",
    "g = sns.scatterplot(x='dates', y='rh', data=df, hue='dates', palette='colorblind', legend=False)\n",
    "g.set_ylim(8.1,5.5)\n",
    "g.set_ylabel('Reflector Height (m)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more reasonable result is obtained with a 0.25-meter median filter and the 12-track requirement. The output daily averages are saved with the txtfile parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_parameters.daily_avg(station, medfilter=.25, ReqTracks=12, plt2screen=False, txtfile='mchn-dailyavg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '^ (?P<year>[ \\d]+) +(?P<doy>[\\d]+) +(?P<rh>[\\d|-|.]+)'\n",
    "filepath = f'/Files/{station}_allRH.txt'\n",
    "data = read_allRH_file(filepath, regex)\n",
    "\n",
    "df = pd.DataFrame(data, index=None, columns=['dates', 'rh'])\n",
    "plt.figure(figsize=(8,8))\n",
    "g = sns.scatterplot(x='dates', y='rh', data=df, hue='dates', palette='colorblind', legend=False)\n",
    "g.set_ylim(7.8,6.6)\n",
    "g.set_ylabel('Reflector Height (m)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '^ (?P<year>[ \\d]+) +(?P<doy>[\\d]+) +(?P<rh>[\\d|-|.]+)'\n",
    "filepath = f'/Files/{station}-dailyavg.txt'\n",
    "data = read_allRH_file(filepath, regex)\n",
    "df = pd.DataFrame(data, index=None, columns=['dates', 'rh'])\n",
    "plt.figure(figsize=(8,8))\n",
    "g = sns.scatterplot(x='dates', y='rh', data=df, legend=False)\n",
    "g.set_ylim(7.6,6.75)\n",
    "g.set_ylabel('Reflector Height (m)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tracks required will depend on the site. Here the azimuth is restricted because mchn is on the coastline of Lake Superior, so the azimuth mask will limit the number of satellite tracks available.\n",
    "Please note that these reflections are from ice in the winter and water during the summer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is a [tide gauge](https://tides.gc.ca/eng/Station/Month?sid=10750) at this site. The tide gauge is operated by the Canadian Hydrographic Service within Fisheries and Ocean Canada.  The tide data can be downloaded from [this link](http://www.isdm-gdsi.gc.ca/isdm-gdsi/twl-mne/inventory-inventaire/interval-intervalle-eng.asp?user=isdm-gdsi&region=CA&tst=1&no=10750).  Please select the daily mean water level, as there are restrictions on hourly data (more information is available on the download page).  A csv file is generated after a request is submitted.\n",
    "\n",
    "The daily mean water level during 2013 looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tide_file(file, regex):\n",
    "    #set up empty lists\n",
    "    data = {'tidedates': [], 'waterlevel': []}\n",
    "\n",
    "    #read tide data \n",
    "    with open(f'{refl_code_dir}{file}', 'r') as myfile:\n",
    "        data = myfile.read()\n",
    "        matches = re.finditer(regex, data, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "        for match in matches:\n",
    "            data['tidedates'].append(datetime.strptime(match.group('date'), '%Y-%m-%d'))\n",
    "            data['waterlevel'].append(float(match.group('slev')))\n",
    "\n",
    "    return data\n",
    "\n",
    "#both tidegauge and gps data sets have gaps, so pad the missing days with nan (also generate a date vector for the full year)\n",
    "\n",
    "def addnans(ymdvec, datavec):\n",
    "    yearvec=[]\n",
    "    paddedvec=[]\n",
    "    date1 = datetime.fromisoformat('2013-01-01')\n",
    "    date2 = datetime.fromisoformat('2013-12-31')\n",
    "    ct_day = date1\n",
    "    \n",
    "    for day in range(0, 365):\n",
    "        yearvec.append(ct_day)\n",
    "        ct_day = ct_day + timedelta(days=1)\n",
    "        \n",
    "    tmp = math.nan\n",
    "    for i in range(0, len(yearvec)):\n",
    "        for j in range(0, len(ymdvec)):\n",
    "            if yearvec[i] == ymdvec[j]:\n",
    "                tmp = datavec[j]\n",
    "        paddedvec.append(tmp)\n",
    "        tmp = math.nan\n",
    "\n",
    "    return yearvec, paddedvec\n",
    "\n",
    "            \n",
    "def getrms(residuals):\n",
    "    val = np.sqrt((residuals**2).mean())\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEED TO FIX ALL THIS\n",
    "\n",
    "\n",
    "regex = '^ (?P<year>[ \\d]+) +(?P<doy>[\\d]+) +(?P<rh>[\\d|-|.]+)'\n",
    "filepath = f'/Files/{station}-dailyavg.txt'\n",
    "data = read_allRH_file(filepath, regex)\n",
    "df = pd.DataFrame(data, index=None, columns=['dates', 'rh'])\n",
    "# plt.figure(figsize=(8,8))\n",
    "# g = sns.scatterplot(x='dates', y='rh', data=df, legend=False)\n",
    "# g.set_ylim(7.6,6.75)\n",
    "# g.set_ylabel('Reflector Height (m)')\n",
    "\n",
    "tide_file_path = '10750-01-JAN-2013_slev.csv'\n",
    "regex = '(?P<date>^[\\d|-]+),(?P<slev>[-|\\d|.]*),'\n",
    "    \n",
    "data = read_tide_files(tide_file_path, regex)\n",
    "df = pd.DataFrame(data, index=None, columns=['Dates', 'Water Level'])\n",
    "\n",
    "#pad missing days with nan\n",
    "ymd, padded_rh = addnans(mchndates, reflht)\n",
    "ymd, padded_wl = addnans(tidedates, waterlevel)\n",
    "\n",
    "    #create numpy array objects\n",
    "    rh_array = np.array(padded_rh)\n",
    "    wl_array =np.array(padded_wl)\n",
    "\n",
    "    #get linear regression (use scipy but mask out nans)\n",
    "    mask = ~np.isnan(rh_array) & ~np.isnan(wl_array)\n",
    "    \n",
    "    slope, intercept, r_val, p_val, std_err = stats.linregress(rh_array[mask], wl_array[mask])\n",
    "    checkfit = slope*rh_array[mask] + intercept\n",
    "    \n",
    "    resids = [i - j for i, j in zip(wl_array[mask], checkfit)]\n",
    "    resids_array = np.array(resids)\n",
    "    rms_resids = getrms(resids_array)\n",
    "    slope = float(\"{0:.2f}\".format(slope))\n",
    "    intercept = float(\"{0:.2f}\".format(intercept))\n",
    "    r_val = float(\"{0:.3f}\".format(r_val))\n",
    "    p_val = float(\"{0:.2f}\".format(p_val))\n",
    "    std_err = float(\"{0:.3f}\".format(std_err))\n",
    "    rms_resids = float(\"{0:.3f}\".format(rms_resids))\n",
    "\n",
    "    #plot reflector height vs. water level (using masked values)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    ax.plot(rh_array[mask], checkfit, '-', color='black')\n",
    "    ax.scatter(rh_array[mask], wl_array[mask], color = 'tab:blue')\n",
    "    \n",
    "    ax.set_xlabel(\"Reflector Height (m)\", fontsize=16)\n",
    "    ax.set_ylabel(\"Water Level (m)\", fontsize=16)\n",
    "    ax.set_title('MCHN Reflector Height vs. Tide Gauge Measurements', fontsize=18)\n",
    "    txtstr = '\\n'.join((\n",
    "        'Slope=%.2f' % (slope, ),\n",
    "        'Intercept=%.2f' % (intercept, ),\n",
    "        'Correlation=%.3f' % (r_val, ),\n",
    "        'P-value=%.2f' % (p_val, ),\n",
    "        'RMS of Residuals=%.3f' % (rms_resids, )))\n",
    "    props = dict(boxstyle='round', facecolor='tab:blue', alpha=0.5)\n",
    "    ax.text(.65, .95, txtstr, transform=ax.transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "# plot time series for the water levels and reflector heights (with reversed axes)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Date', fontsize=16)\n",
    "    ax1.set_ylabel('Tide Gauge Water Level (m)', color='black', fontsize=16)\n",
    "    ax1.scatter(tidedates, waterlevel, label='Tide Gauge', color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    plt.ylim(-.5,.3)\n",
    "    plt.grid()\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:orange'\n",
    "    ax2.set_ylabel('GPS Reflector Height (m)', color='black', fontsize=16)  # we already handled the x-label with ax1\n",
    "    ax2.scatter(mchndates, reflht, label='GPS Reflector Height', color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor='black')\n",
    "    myFmt = mdates.DateFormatter('%Y-%m-%d')\n",
    "    ax1.xaxis.set_major_formatter(myFmt)\n",
    "    \n",
    "    plt.ylim(6.75,7.55)\n",
    "    plt.gca().invert_yaxis()\n",
    "    fig.legend(loc='lower right', bbox_to_anchor=(0.93, 0.08), edgecolor='black')\n",
    "    plt.title('MCHN Tide Gauge Measurements vs. Reflectometry', fontsize=18)\n",
    "    plt.grid()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnssrefl_jupyter",
   "language": "python",
   "name": "gnssrefl_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
